{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc5c730",
   "metadata": {},
   "source": [
    "# CEGE0004: Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf4785",
   "metadata": {},
   "source": [
    "## Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aec916",
   "metadata": {},
   "source": [
    "In this notebook, a multi-layer perceptron classifier is implemented in both pytorch and scikit-learn. That classifier is used to predict the default of credit card client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5dc34",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8129d9",
   "metadata": {},
   "source": [
    "I am working with the default of credit card clients Data Set from UCI Machine Learning Repository, to to classify and predict customer defaults. The link of dataset is shown below:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    "The dataset employes a binary variable, default (Yes = 1, No = 0), as the response variable. At the same time, the following 23 variables are used as explanatory variables:\n",
    "\n",
    "X1: Amount of the given credit: it includes both the individual consumer credit and his/her family credit.\n",
    "\n",
    "X2: Gender (1 = male; 2 = female).\n",
    "\n",
    "X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
    "\n",
    "X4: Marital status (1 = married; 2 = single; 3 = others).\n",
    "\n",
    "X5: Age (year).\n",
    "\n",
    "X6 - X11: History of past payment from April to September. (e.g. X6 = the repayment status in September; X7 = the repayment status in August; . . .) The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; . . .; 9 = payment delay for nine months and above.\n",
    "\n",
    "X12 - X17: Amount of bill statement. (e.g. X12 = amount of bill statement in September; X13 = amount of bill statement in August; . . .).\n",
    "\n",
    "X18 - X23: Amount of previous payment. (e.g. X18 = amount paid in September; X19 = amount paid in August, 2005; . . .)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4fe3cf",
   "metadata": {},
   "source": [
    "### Read the pretreated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3e58cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set shape:\t (24000, 23)\n",
      "test set shape:\t\t (6000, 23)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the dataset into a Pandas DataFrame\n",
    "xs_train = pd.read_csv(\"x_train.csv\")\n",
    "xs_test = pd.read_csv(\"x_test.csv\")\n",
    "\n",
    "ys_train = pd.read_csv(\"y_train.csv\")\n",
    "ys_train = ys_train['default payment next month']\n",
    "ys_test = pd.read_csv(\"y_test.csv\")\n",
    "ys_test = ys_test['default payment next month']\n",
    "\n",
    "# print the shape of the training set\n",
    "print('training set shape:\\t', xs_train.shape)\n",
    "# print the shape of the test set\n",
    "print('test set shape:\\t\\t', xs_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc16b0",
   "metadata": {},
   "source": [
    "## Preparation for the model based on pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8f69f",
   "metadata": {},
   "source": [
    "Here a multi-layer perceptron neural network is established with pytorch environment. Some codes are referenced from: https://github.com/aldolipani/CEGE0004/blob/master/5%20-%20Week/neural_networks.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f872ab",
   "metadata": {},
   "source": [
    "### Importing the required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0278b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c92bac",
   "metadata": {},
   "source": [
    "### Define data handling function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be6367",
   "metadata": {},
   "source": [
    "The BinaryDataset class is designed to handle input features and binary target labels.\n",
    "It converts the input data (xs) into a float tensor and target labels (ys) into a long tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e5879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class binarydataset(Dataset):\n",
    "    '''\n",
    "    A custom dataset class for binary classification tasks that extends the PyTorch Dataset class.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): The input features in a pandas DataFrame.\n",
    "        y (pd.DataFrame): The binary target labels in a pandas DataFrame.\n",
    "    \n",
    "    The BinaryDataset class is designed to handle input features and binary target labels.\n",
    "    It converts the input data (X) into a float tensor and target labels (y) into a long tensor.\n",
    "    '''\n",
    "    def __init__(self, X, y):\n",
    "        # convert X to a float tensor and store it in self.X\n",
    "        self.X = torch.tensor(X.values.astype(np.float32), dtype=torch.float32)\n",
    "        # convert y to a long tensor and store it in self.y\n",
    "        self.y = torch.tensor(y.values.astype(np.float32), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the length of the input tensor\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return the X and y tensors at the given index\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc5b49",
   "metadata": {},
   "source": [
    "### Define the multi-layer perceptron neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcce3d8",
   "metadata": {},
   "source": [
    "The MLP class consists of an input layer, one or more hidden layers, and an output layer. It uses the ReLU activation function for the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2bd1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    A custom Multilayer Perceptron (MLP) class that extends the PyTorch nn.Module class.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The number of neurons in the input layer, which depends on the dataset.\n",
    "        hidden_sizes (list): A list of integers representing the number of neurons in each hidden layer.\n",
    "        output_size (int): The number of neurons in the output layer.\n",
    "\n",
    "    The MLP class consists of an input layer, one or more hidden layers, and an output layer.\n",
    "    It uses the ReLU activation function for the hidden layers.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList() # list of layers\n",
    "        next_hidden_size = input_size # input size of the first layer, this depends on the dataset\n",
    "        for hidden_size in hidden_sizes: # for loop to define the next hidden layers\n",
    "            self.layers.append(nn.Linear(next_hidden_size, hidden_size))\n",
    "            next_hidden_size = hidden_size\n",
    "        self.output = nn.Linear(next_hidden_size, output_size) # output layer\n",
    "\n",
    "    def forward(self, x): \n",
    "        '''\n",
    "        Defines the forward pass for the MLP model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            out (torch.Tensor): The output tensor after passing through the MLP layers and activation functions.\n",
    "        '''\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            out = nn.functional.relu(out) # activation function\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159dfad8",
   "metadata": {},
   "source": [
    "### Define the loss computing function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41430b20",
   "metadata": {},
   "source": [
    "This function computes the loss on a given test set using a trained model and a loss criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "159995fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_test(model, test_loader, criterium):\n",
    "    '''\n",
    "    This function computes the loss on a given test set using a trained model and a loss criterion.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model to use for prediction.\n",
    "        test_loader (DataLoader): The DataLoader containing the test set.\n",
    "        criterium (nn.Module): The loss criterion to use.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss over the test set.\n",
    "    '''\n",
    "    tot_loss = 0\n",
    "    batch_size = test_loader.batch_size\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        # Extract input and output data from the batch.\n",
    "        xs, ys = batch\n",
    "        # Move the data to the device (GPU).\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "        # generate the predictions using the model\n",
    "        pred_ys = model(xs)\n",
    "        # evaluate the predictions using the loss criterion\n",
    "        loss = criterium(pred_ys, ys)\n",
    "        # get the loss value and sum it to the total loss\n",
    "        tot_loss += loss.item()\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            break\n",
    "    # normalize the loss based on the number of testing examples\n",
    "    loss = tot_loss / batch_size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb76f1",
   "metadata": {},
   "source": [
    "### Define the accuracy computing function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7270b6",
   "metadata": {},
   "source": [
    "This function computes the accuracy of a given data set using a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44cbc557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data_loader):\n",
    "    '''\n",
    "    This function computes the accuracy of a given data set using a trained model.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): The DataLoader containing the data set.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the given data set.\n",
    "    '''\n",
    "    # here we count the correct answers\n",
    "    correct = 0.0\n",
    "    # here we count all the answers\n",
    "    total = 0.0\n",
    "    # ignore the gradient graph\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Extract input and output data from the batch.\n",
    "            xs, ys = batch\n",
    "            # Move the data to the device (GPU).\n",
    "            hat_ys = model(xs.to(device)).detach().cpu()\n",
    "            # get the predicted classes\n",
    "            _, hat_ys = torch.max(hat_ys, 1)\n",
    "            # increment the correct count if prediction matches the ground truth\n",
    "            correct += (hat_ys == ys).sum()\n",
    "            # increment the total count by the number of examples in the batch\n",
    "            total += ys.size(0)\n",
    "\n",
    "    # return the accuracy as the ratio of correct to total counts\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f592b546",
   "metadata": {},
   "source": [
    "### Define the model training function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759479de",
   "metadata": {},
   "source": [
    "This function initializes the MLP model with the given hyperparameters, creates an optimizer and a loss function, and then trains the model using the input data. The training process is performed for a fixed number of epochs.\n",
    "\n",
    "During training, the function records the training and testing losses every 200 batches and prints the current progress. After training is complete, the function returns the trained model, DataLoader instances for the training and testing sets, and the lists of training and testing losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe069d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(params):\n",
    "    '''\n",
    "    Trains an MLP model with the given hyperparameters and data.\n",
    "\n",
    "    Args:\n",
    "        params (dict): A dictionary containing the following keys:\n",
    "            - 'hidden_sizes' (list): A list of integers representing the number of neurons in each hidden layer.\n",
    "            - 'lr' (float): The learning rate for the optimizer.\n",
    "            - 'weight_decay' (float): The weight decay (L2 penalty) for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Module): The trained MLP model.\n",
    "        train_loader (DataLoader): DataLoader instance for the training set.\n",
    "        test_loader (DataLoader): DataLoader instance for the testing set.\n",
    "        training_loss (list): A list of training losses recorded every 200 batches.\n",
    "        testing_loss (list): A list of testing losses recorded every 200 batches.\n",
    "\n",
    "    This function initializes the MLP model with the given hyperparameters, creates an optimizer and a loss function,\n",
    "    and then trains the model using the input data. The training process is performed for a fixed number of epochs.\n",
    "    During training, the function records the training and testing losses every 200 batches and prints the current\n",
    "    progress. After training is complete, the function returns the trained model, DataLoader instances for the\n",
    "    training and testing sets, and the lists of training and testing losses.\n",
    "    '''\n",
    "    # initialize the model with the current set of hyperparameters\n",
    "    model = MLP(23, params['hidden_sizes'], 2)\n",
    "\n",
    "    # move the model to the device (GPU)\n",
    "    model.to(device)\n",
    "    # set optimizer and criterium\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=params['lr'],\n",
    "                                momentum=0.99,\n",
    "                                weight_decay=params['weight_decay'])\n",
    "    criterium = nn.CrossEntropyLoss()\n",
    "\n",
    "    # set the batch size for training\n",
    "    batch_size = 20\n",
    "    # use the DataLoader helper to generate an iterator for the train set to return random examples 5 by 5\n",
    "    train_loader = DataLoader(traindata, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    # use the DataLoader helper to generate an iterator for the test set to return examples 100 by 100\n",
    "    test_loader = DataLoader(testdata, batch_size=100, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # set the number of epochs for training\n",
    "    epochs = 5\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # These variables are used to store the losses.\n",
    "    running_loss = 0\n",
    "    training_loss = []\n",
    "    testing_loss = []\n",
    "    print(f'Testing model for parameters {params}!')\n",
    "    # train the model for a fixed number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over the batches.\n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            # Extract input and output data from the batch.\n",
    "            xs, ys = batch\n",
    "            # Move the data to the device (GPU).\n",
    "            xs, ys = xs.to(device), ys.to(device)\n",
    "            # Reset the gradients.\n",
    "            optimizer.zero_grad()\n",
    "            # Generate the predictions.\n",
    "            pred_ys = model(xs)\n",
    "            # Compute the loss.\n",
    "            loss = criterium(pred_ys, ys)\n",
    "            # Backpropagation.\n",
    "            loss.backward()\n",
    "            # Gradient clipping.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            # Update the model parameters.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the running loss.\n",
    "            running_loss += loss.item()\n",
    "            # Every 200 batches, print statistics about the training.\n",
    "            if (i + 1) % 200 == 0:\n",
    "                # Compute the average training loss on the last 200 batches.\n",
    "                running_loss /= 200\n",
    "                # Keep track of the training loss.\n",
    "                training_loss.append(running_loss)\n",
    "                # Set the model to evaluation mode.\n",
    "                model.eval()\n",
    "                # Compute the validation loss.\n",
    "                test_loss = compute_loss_test(model, test_loader, criterium)\n",
    "                # Keep track of the validation loss.\n",
    "                testing_loss.append(test_loss)\n",
    "                # Set the model back to training mode.\n",
    "                model.train()\n",
    "                # Print the training and validation losses.\n",
    "                print('Epoch [%d/%d], Step [%d/%d], Train Loss: %.4f, Test Loss: %.4f' % (\n",
    "                    epoch + 1,\n",
    "                    epochs,\n",
    "                    i + 1,\n",
    "                    len(xs_train) // batch_size,\n",
    "                    running_loss,\n",
    "                    test_loss))\n",
    "                # Reset the running loss.\n",
    "                running_loss = 0\n",
    "                \n",
    "    return model, train_loader, test_loader, training_loss, testing_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ac843",
   "metadata": {},
   "source": [
    "### Set dataset and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c8dbeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows for multiprocessing on Windows\n",
    "multiprocessing.freeze_support()\n",
    "\n",
    "# create a training dataset object\n",
    "traindata = binarydataset(xs_train, ys_train)\n",
    "# create a test dataset object\n",
    "testdata = binarydataset(xs_test, ys_test)\n",
    "\n",
    "# check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef777726",
   "metadata": {},
   "source": [
    "## Train the model with random hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca7b47ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model for parameters {'hidden_sizes': (15, 15, 2), 'lr': 1e-05, 'weight_decay': 0.0001}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 273.8235, Test Loss: 106.3747\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 102.3678, Test Loss: 47.4283\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 62.7847, Test Loss: 35.1630\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 55.1259, Test Loss: 24.8968\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 36.6609, Test Loss: 12.0003\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 14.8619, Test Loss: 4.8194\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 9.9600, Test Loss: 2.5969\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 2.8739, Test Loss: 1.7749\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 3.1038, Test Loss: 1.4540\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 2.7915, Test Loss: 1.1266\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 3.8360, Test Loss: 0.9427\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 1.6322, Test Loss: 0.8278\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 1.1787, Test Loss: 0.7829\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.9445, Test Loss: 0.7848\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.9835, Test Loss: 0.7494\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 1.3991, Test Loss: 0.6840\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 1.6989, Test Loss: 0.6618\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 1.2067, Test Loss: 0.6160\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.8188, Test Loss: 0.6013\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 1.4861, Test Loss: 0.6075\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 1.1743, Test Loss: 0.6188\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.8418, Test Loss: 0.5751\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.7048, Test Loss: 0.5551\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.8401, Test Loss: 0.5630\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.9488, Test Loss: 0.5427\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.8447, Test Loss: 0.5472\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.7613, Test Loss: 0.5191\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.6864, Test Loss: 0.5167\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.7863, Test Loss: 0.5214\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 1.3997, Test Loss: 0.5179\n"
     ]
    }
   ],
   "source": [
    "# train the model with hyper parameters\n",
    "params = {'hidden_sizes': (15, 15, 2), 'lr': 0.00001, 'weight_decay': 0.0001}\n",
    "model, train_loader, test_loader, training_loss, testing_loss = train_model(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64587a0a",
   "metadata": {},
   "source": [
    "### Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0c4dbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy of the MLP: 0.777\n",
      "Test accuracy of the MLP: 0.784\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode.\n",
    "model.eval()\n",
    "# Compute the training accuracy.\n",
    "train_accuracy = accuracy(train_loader)\n",
    "print('Train accuracy of the MLP: {:.3f}'.format(train_accuracy))\n",
    "# Compute the testing accuracy.\n",
    "test_accuracy = accuracy(test_loader)\n",
    "print('Test accuracy of the MLP: {:.3f}'.format(test_accuracy))\n",
    "# Name the accuracy for plotting\n",
    "accuracy1 = test_accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84f39e",
   "metadata": {},
   "source": [
    "The accuracy is low, because the hyper parameters applied may not be the best option. In the following part, hyper parameters are tuned to determine the best groups, in order to increase the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8d93e",
   "metadata": {},
   "source": [
    "## Determine best hyper parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4181324",
   "metadata": {},
   "source": [
    "### Define the hyper parameters to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757578f8",
   "metadata": {},
   "source": [
    "The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. (Reference from: \n",
    "https://stackoverflow.com/questions/15550606/neural-networks-input-and-output-layers.)\n",
    "Learning rate and weight decay were picked after rough trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73fd39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters to tune\n",
    "param_grid = {'hidden_sizes': [(15, 15, 2), (20, 20, 2), (25, 25, 2)],\n",
    "                        'lr': [0.001, 0.0001, 0.00001],\n",
    "              'weight_decay': [0.01, 0.0001, 0.00001]}\n",
    "\n",
    "# create a list of all possible combinations of hyperparameters\n",
    "param_list = list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68661db8",
   "metadata": {},
   "source": [
    "### Hyper parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cdb506",
   "metadata": {},
   "source": [
    "The computation here was massive, thus the model was operated by GPU to speed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cd842fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model for parameters {'hidden_sizes': (15, 15, 2), 'lr': 0.001, 'weight_decay': 0.01}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 58.8546, Test Loss: 0.3133\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.5462, Test Loss: 0.3169\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5228, Test Loss: 0.3133\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5264, Test Loss: 0.3142\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5252, Test Loss: 0.3134\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5202, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5287, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5354, Test Loss: 0.3139\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5409, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5324, Test Loss: 0.3137\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5201, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5245, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5285, Test Loss: 0.3135\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5222, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5430, Test Loss: 0.3146\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5253, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5343, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5293, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5259, Test Loss: 0.3140\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5342, Test Loss: 0.3136\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5338, Test Loss: 0.3138\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5308, Test Loss: 0.3139\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5309, Test Loss: 0.3135\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5273, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5253, Test Loss: 0.3134\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5347, Test Loss: 0.3146\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5392, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5232, Test Loss: 0.3136\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5175, Test Loss: 0.3133\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5429, Test Loss: 0.3139\n",
      "Best parameter is updated to {'hidden_sizes': (15, 15, 2), 'lr': 0.001, 'weight_decay': 0.01}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (15, 15, 2), 'lr': 0.001, 'weight_decay': 0.0001}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 23.4027, Test Loss: 0.3151\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.5408, Test Loss: 0.3144\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5188, Test Loss: 0.3139\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5283, Test Loss: 0.3135\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5343, Test Loss: 0.3134\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5454, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5368, Test Loss: 0.3144\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5351, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5263, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5226, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5275, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5337, Test Loss: 0.3135\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5381, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5186, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5259, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5353, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5333, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5317, Test Loss: 0.3136\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5290, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5253, Test Loss: 0.3143\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5387, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5300, Test Loss: 0.3135\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5318, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5286, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5355, Test Loss: 0.3134\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5228, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5308, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5269, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5253, Test Loss: 0.3135\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5414, Test Loss: 0.3147\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (15, 15, 2), 'lr': 0.001, 'weight_decay': 1e-05}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 14.0981, Test Loss: 0.3154\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.5420, Test Loss: 0.3156\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5348, Test Loss: 0.3133\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5244, Test Loss: 0.3137\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5226, Test Loss: 0.3132\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5175, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5376, Test Loss: 0.3141\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5315, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5233, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5249, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5301, Test Loss: 0.3136\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5347, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5344, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5282, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5272, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5425, Test Loss: 0.3150\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5239, Test Loss: 0.3134\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5273, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5282, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5133, Test Loss: 0.3140\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5370, Test Loss: 0.3148\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5267, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5280, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5494, Test Loss: 0.3138\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5255, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5363, Test Loss: 0.3139\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5299, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5390, Test Loss: 0.3138\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5123, Test Loss: 0.3136\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5387, Test Loss: 0.3146\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (15, 15, 2), 'lr': 0.0001, 'weight_decay': 0.01}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 5.1446, Test Loss: 0.7240\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.6457, Test Loss: 0.3806\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5445, Test Loss: 0.3275\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.6363, Test Loss: 0.3145\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5500, Test Loss: 0.3134\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.8675, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5841, Test Loss: 0.3137\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5256, Test Loss: 0.3140\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5431, Test Loss: 0.3137\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5327, Test Loss: 0.3138\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.6848, Test Loss: 0.3145\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5177, Test Loss: 0.3174\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5362, Test Loss: 0.3288\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5754, Test Loss: 0.3557\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5312, Test Loss: 0.3807\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.7329, Test Loss: 0.3380\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.6041, Test Loss: 0.3900\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.7821, Test Loss: 0.6564\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.7924, Test Loss: 0.4106\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.6303, Test Loss: 0.3865\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.7496, Test Loss: 0.3347\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.6093, Test Loss: 0.3920\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.6140, Test Loss: 0.3559\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5386, Test Loss: 0.3579\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5542, Test Loss: 0.3419\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5575, Test Loss: 0.3494\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5487, Test Loss: 0.3083\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5369, Test Loss: 0.3218\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5462, Test Loss: 0.3511\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5682, Test Loss: 0.3218\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (15, 15, 2), 'lr': 0.0001, 'weight_decay': 0.0001}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 18.3696, Test Loss: 0.3695\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.5735, Test Loss: 0.3211\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5348, Test Loss: 0.3138\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5307, Test Loss: 0.3133\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5238, Test Loss: 0.3132\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5311, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5522, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5282, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5233, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5216, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5257, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5311, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5284, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5311, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5413, Test Loss: 0.3134\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5172, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5410, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5220, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5282, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5104, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5306, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5301, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5476, Test Loss: 0.3135\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5332, Test Loss: 0.3135\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5327, Test Loss: 0.3134\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5317, Test Loss: 0.3134\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5152, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5330, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5266, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5413, Test Loss: 0.3133\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (15, 15, 2), 'lr': 0.0001, 'weight_decay': 1e-05}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 0.7514, Test Loss: 0.3742\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.5606, Test Loss: 0.3190\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5315, Test Loss: 0.3136\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5304, Test Loss: 0.3139\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5300, Test Loss: 0.3139\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5304, Test Loss: 0.3137\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5308, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5280, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5342, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5348, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5270, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5257, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5427, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5136, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5266, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5301, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5345, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5332, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5351, Test Loss: 0.3135\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5398, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5246, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5394, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5233, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5182, Test Loss: 0.3133\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5444, Test Loss: 0.3134\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5288, Test Loss: 0.3136\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5268, Test Loss: 0.3134\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5342, Test Loss: 0.3135\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5375, Test Loss: 0.3136\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5096, Test Loss: 0.3134\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (15, 15, 2), 'lr': 1e-05, 'weight_decay': 0.01}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 1289.9290, Test Loss: 316.5432\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 141.8915, Test Loss: 1.8768\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 1.9790, Test Loss: 0.6376\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 1.3374, Test Loss: 0.4412\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 1.0643, Test Loss: 0.3707\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5988, Test Loss: 0.3568\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.6076, Test Loss: 0.3498\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.7212, Test Loss: 0.3448\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5867, Test Loss: 0.3394\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5667, Test Loss: 0.3345\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5829, Test Loss: 0.3310\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5979, Test Loss: 0.3285\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5441, Test Loss: 0.3265\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5877, Test Loss: 0.3248\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5521, Test Loss: 0.3234\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5441, Test Loss: 0.3222\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5406, Test Loss: 0.3212\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5927, Test Loss: 0.3203\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5450, Test Loss: 0.3194\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5365, Test Loss: 0.3187\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5665, Test Loss: 0.3182\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5485, Test Loss: 0.3176\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5382, Test Loss: 0.3171\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5316, Test Loss: 0.3167\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5525, Test Loss: 0.3164\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5583, Test Loss: 0.3161\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5347, Test Loss: 0.3157\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5360, Test Loss: 0.3154\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5240, Test Loss: 0.3152\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5336, Test Loss: 0.3150\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (15, 15, 2), 'lr': 1e-05, 'weight_decay': 0.0001}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 62.7047, Test Loss: 0.8578\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.9320, Test Loss: 0.5138\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.7525, Test Loss: 0.4981\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.8398, Test Loss: 0.4563\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.7694, Test Loss: 0.4024\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.6786, Test Loss: 0.3913\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.6847, Test Loss: 0.3853\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.7285, Test Loss: 0.4680\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.6883, Test Loss: 0.3714\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.6395, Test Loss: 0.4471\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.6393, Test Loss: 0.4261\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.7105, Test Loss: 0.4609\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.6990, Test Loss: 0.4019\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.6127, Test Loss: 0.3616\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.6431, Test Loss: 0.3603\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.6021, Test Loss: 0.3588\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5874, Test Loss: 0.3631\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.6163, Test Loss: 0.3796\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.6316, Test Loss: 0.3587\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.6205, Test Loss: 0.3770\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.6123, Test Loss: 0.3624\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5938, Test Loss: 0.3865\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.6387, Test Loss: 0.3788\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.6277, Test Loss: 0.3752\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.6425, Test Loss: 0.3581\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.6187, Test Loss: 0.4420\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.7027, Test Loss: 0.3692\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.6285, Test Loss: 0.3984\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.6100, Test Loss: 0.3695\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.6486, Test Loss: 0.3570\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (15, 15, 2), 'lr': 1e-05, 'weight_decay': 1e-05}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 191.9283, Test Loss: 25.6635\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 26.9151, Test Loss: 11.7718\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 6.7464, Test Loss: 1.7134\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 2.4199, Test Loss: 0.9655\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 1.0748, Test Loss: 0.6464\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 1.0230, Test Loss: 0.5755\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.9367, Test Loss: 0.6619\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.9748, Test Loss: 0.5324\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.8570, Test Loss: 0.6209\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 1.1018, Test Loss: 0.5289\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.8854, Test Loss: 0.5638\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.8599, Test Loss: 0.5182\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.9338, Test Loss: 0.7031\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.8574, Test Loss: 0.5013\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.9378, Test Loss: 0.5092\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.8717, Test Loss: 0.5908\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 1.0427, Test Loss: 0.5201\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.9197, Test Loss: 0.6117\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 1.2453, Test Loss: 0.6890\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 1.1221, Test Loss: 0.5008\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.9374, Test Loss: 0.5222\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.8917, Test Loss: 0.5404\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.7886, Test Loss: 0.5446\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.7928, Test Loss: 0.5328\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 1.0611, Test Loss: 0.5075\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.8292, Test Loss: 0.4840\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.8006, Test Loss: 0.5029\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.8348, Test Loss: 0.5983\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.8641, Test Loss: 0.5317\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.8196, Test Loss: 0.4775\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (20, 20, 2), 'lr': 0.001, 'weight_decay': 0.01}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 5.4236, Test Loss: 0.3152\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.5694, Test Loss: 0.3148\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5297, Test Loss: 0.3145\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5432, Test Loss: 0.3151\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5278, Test Loss: 0.3133\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5301, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5133, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5282, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5408, Test Loss: 0.3148\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5263, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5383, Test Loss: 0.3138\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5377, Test Loss: 0.3151\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5222, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5347, Test Loss: 0.3140\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5413, Test Loss: 0.3138\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5286, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5352, Test Loss: 0.3135\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5205, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5295, Test Loss: 0.3135\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5115, Test Loss: 0.3135\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5324, Test Loss: 0.3157\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5337, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5386, Test Loss: 0.3144\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5376, Test Loss: 0.3139\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5208, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5289, Test Loss: 0.3139\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5258, Test Loss: 0.3134\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5415, Test Loss: 0.3136\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5270, Test Loss: 0.3133\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5389, Test Loss: 0.3148\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (20, 20, 2), 'lr': 0.001, 'weight_decay': 0.0001}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 12.4805, Test Loss: 0.3375\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.5313, Test Loss: 0.3184\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5309, Test Loss: 0.3137\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5383, Test Loss: 0.3136\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5317, Test Loss: 0.3132\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5246, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5278, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5477, Test Loss: 0.3150\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5205, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5318, Test Loss: 0.3135\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5257, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5294, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5306, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5233, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5320, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5243, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5407, Test Loss: 0.3137\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5324, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5360, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5459, Test Loss: 0.3142\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5231, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5372, Test Loss: 0.3135\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5254, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5174, Test Loss: 0.3135\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5109, Test Loss: 0.3137\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5367, Test Loss: 0.3142\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5507, Test Loss: 0.3139\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5249, Test Loss: 0.3133\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5336, Test Loss: 0.3135\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5247, Test Loss: 0.3135\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (20, 20, 2), 'lr': 0.001, 'weight_decay': 1e-05}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 1.0858, Test Loss: 0.3442\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.5623, Test Loss: 0.3206\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5409, Test Loss: 0.3496\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5348, Test Loss: 0.3133\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5232, Test Loss: 0.3132\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5394, Test Loss: 0.3143\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5463, Test Loss: 0.3136\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 1.8549, Test Loss: 0.3161\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.7306, Test Loss: 0.3136\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5224, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5357, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5392, Test Loss: 0.3143\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5201, Test Loss: 0.3134\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5256, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5377, Test Loss: 0.3148\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5248, Test Loss: 0.3138\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5348, Test Loss: 0.3135\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5398, Test Loss: 0.3135\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5191, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5277, Test Loss: 0.3136\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5300, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5327, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5337, Test Loss: 0.3143\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5399, Test Loss: 0.3137\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5346, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5357, Test Loss: 0.3135\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5224, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5393, Test Loss: 0.3139\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5236, Test Loss: 0.3134\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5267, Test Loss: 0.3133\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (20, 20, 2), 'lr': 0.0001, 'weight_decay': 0.01}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 126.4563, Test Loss: 0.4045\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.6443, Test Loss: 0.3682\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.6644, Test Loss: 0.4210\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.7183, Test Loss: 0.7618\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 1.0283, Test Loss: 0.5536\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.7213, Test Loss: 0.5423\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.7784, Test Loss: 0.3197\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5988, Test Loss: 0.3274\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.6530, Test Loss: 0.4000\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.6519, Test Loss: 0.3156\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.7099, Test Loss: 0.3243\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.6111, Test Loss: 0.3548\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5624, Test Loss: 0.3275\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5823, Test Loss: 0.3365\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.6158, Test Loss: 0.3258\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.6297, Test Loss: 0.3099\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.6022, Test Loss: 0.3488\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5638, Test Loss: 0.3246\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5797, Test Loss: 0.3323\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5444, Test Loss: 0.3110\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5476, Test Loss: 0.3203\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5723, Test Loss: 0.3596\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5368, Test Loss: 0.4025\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5898, Test Loss: 0.3122\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5450, Test Loss: 0.3712\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.6081, Test Loss: 0.3340\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.6181, Test Loss: 0.3313\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5502, Test Loss: 0.3141\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5769, Test Loss: 0.3155\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5360, Test Loss: 0.3184\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (20, 20, 2), 'lr': 0.0001, 'weight_decay': 0.0001}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 14.5593, Test Loss: 0.4203\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.6002, Test Loss: 0.3241\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5308, Test Loss: 0.3135\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5332, Test Loss: 0.3132\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5352, Test Loss: 0.3132\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5322, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5285, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5351, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5292, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5191, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5353, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5332, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5116, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5269, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5221, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5372, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5524, Test Loss: 0.3135\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5300, Test Loss: 0.3136\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5312, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5218, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5244, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5345, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5351, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5335, Test Loss: 0.3133\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5245, Test Loss: 0.3133\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5140, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5359, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5342, Test Loss: 0.3133\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5388, Test Loss: 0.3134\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5329, Test Loss: 0.3134\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (20, 20, 2), 'lr': 0.0001, 'weight_decay': 1e-05}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 1.7126, Test Loss: 0.6629\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.6144, Test Loss: 0.4398\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5595, Test Loss: 0.3355\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5368, Test Loss: 0.3428\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5685, Test Loss: 0.3289\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5441, Test Loss: 0.3248\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5470, Test Loss: 0.3135\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5861, Test Loss: 0.3136\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5295, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5175, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5320, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5288, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5623, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5338, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5242, Test Loss: 0.3134\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5299, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5425, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5148, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5329, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5343, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5251, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5187, Test Loss: 0.3138\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5849, Test Loss: 0.3137\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5360, Test Loss: 0.3137\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5364, Test Loss: 0.3137\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5152, Test Loss: 0.3133\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5434, Test Loss: 0.3138\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5473, Test Loss: 0.3139\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5367, Test Loss: 0.3135\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5392, Test Loss: 0.3135\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (20, 20, 2), 'lr': 1e-05, 'weight_decay': 0.01}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 412.5672, Test Loss: 58.7791\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 25.6835, Test Loss: 2.0701\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 1.3885, Test Loss: 0.4890\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.7481, Test Loss: 0.4367\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.7087, Test Loss: 0.4147\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.6766, Test Loss: 0.3968\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.6510, Test Loss: 0.3823\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.6322, Test Loss: 0.3705\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.6111, Test Loss: 0.3609\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5987, Test Loss: 0.3532\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5900, Test Loss: 0.3470\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5812, Test Loss: 0.3418\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5732, Test Loss: 0.3376\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5616, Test Loss: 0.3339\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5667, Test Loss: 0.3310\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5641, Test Loss: 0.3291\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5998, Test Loss: 0.3361\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.6018, Test Loss: 0.3423\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.6040, Test Loss: 0.3436\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5834, Test Loss: 0.3747\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.6441, Test Loss: 0.3434\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.6475, Test Loss: 0.3175\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5536, Test Loss: 0.3175\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5636, Test Loss: 0.3251\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.6149, Test Loss: 0.3165\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5612, Test Loss: 0.3144\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5534, Test Loss: 0.3228\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5512, Test Loss: 0.3121\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5841, Test Loss: 0.3344\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5671, Test Loss: 0.3617\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (20, 20, 2), 'lr': 1e-05, 'weight_decay': 0.0001}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 145.5932, Test Loss: 19.5090\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 28.5942, Test Loss: 7.5581\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 11.1900, Test Loss: 3.7440\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 4.4779, Test Loss: 2.0054\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 2.9170, Test Loss: 1.3190\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 1.9024, Test Loss: 1.1306\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 1.5181, Test Loss: 1.1002\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 1.3352, Test Loss: 0.9472\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 1.7054, Test Loss: 0.8250\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 2.6996, Test Loss: 0.7440\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 1.7548, Test Loss: 0.6681\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.9965, Test Loss: 0.6131\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 1.0052, Test Loss: 0.6157\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.8264, Test Loss: 0.5262\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.6896, Test Loss: 0.4972\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 2.5291, Test Loss: 0.4621\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.9601, Test Loss: 0.4599\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.6698, Test Loss: 0.4334\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.7485, Test Loss: 0.4243\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.6490, Test Loss: 0.4150\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 1.1040, Test Loss: 0.3996\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.6902, Test Loss: 0.4125\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.7813, Test Loss: 0.3769\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 1.6558, Test Loss: 0.3746\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.6361, Test Loss: 0.3667\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.7335, Test Loss: 0.4189\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.7105, Test Loss: 0.3596\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 1.5303, Test Loss: 0.3858\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.6645, Test Loss: 0.3683\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 1.0518, Test Loss: 0.3567\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (20, 20, 2), 'lr': 1e-05, 'weight_decay': 1e-05}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 192.9882, Test Loss: 7.2188\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 2.7459, Test Loss: 1.7013\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 1.4844, Test Loss: 1.0410\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 2.5229, Test Loss: 0.9111\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 1.1725, Test Loss: 0.8067\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 1.2708, Test Loss: 0.7035\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 1.0580, Test Loss: 0.6632\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 1.3552, Test Loss: 0.6092\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.7230, Test Loss: 0.5526\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.8185, Test Loss: 0.5570\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.9059, Test Loss: 0.5405\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.8068, Test Loss: 0.5109\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.8145, Test Loss: 0.4785\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.6944, Test Loss: 0.4734\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.8073, Test Loss: 0.5083\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.9226, Test Loss: 0.4451\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5820, Test Loss: 0.4358\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.6531, Test Loss: 0.4259\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.7141, Test Loss: 0.4024\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.6505, Test Loss: 0.3846\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.6287, Test Loss: 0.3905\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.6298, Test Loss: 0.4020\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.6327, Test Loss: 0.4059\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.6729, Test Loss: 0.4166\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.7121, Test Loss: 0.4119\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.7904, Test Loss: 0.3888\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5922, Test Loss: 0.3797\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5864, Test Loss: 0.3749\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5746, Test Loss: 0.3966\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.6109, Test Loss: 0.3833\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (25, 25, 2), 'lr': 0.001, 'weight_decay': 0.01}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 0.8205, Test Loss: 0.3237\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.5174, Test Loss: 0.3145\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5411, Test Loss: 0.3156\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5242, Test Loss: 0.3133\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5422, Test Loss: 0.3153\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5274, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5358, Test Loss: 0.3141\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5340, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5298, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5386, Test Loss: 0.3144\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5130, Test Loss: 0.3135\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5321, Test Loss: 0.3144\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5354, Test Loss: 0.3136\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5430, Test Loss: 0.3146\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5306, Test Loss: 0.3136\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5285, Test Loss: 0.3139\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5174, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5277, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5513, Test Loss: 0.3152\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5401, Test Loss: 0.3136\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5253, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5170, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5129, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5348, Test Loss: 0.3147\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5280, Test Loss: 0.3133\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5306, Test Loss: 0.3136\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5354, Test Loss: 0.3138\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5336, Test Loss: 0.3138\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5227, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5330, Test Loss: 0.3137\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (25, 25, 2), 'lr': 0.001, 'weight_decay': 0.0001}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 25.4210, Test Loss: 0.3728\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.5764, Test Loss: 0.3239\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5397, Test Loss: 0.3136\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5344, Test Loss: 0.3138\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5395, Test Loss: 0.3132\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5165, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5414, Test Loss: 0.3138\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5385, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5336, Test Loss: 0.3138\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5273, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5217, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5186, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5374, Test Loss: 0.3135\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5370, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5321, Test Loss: 0.3134\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5261, Test Loss: 0.3134\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5312, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5192, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5266, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5368, Test Loss: 0.3135\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5281, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5233, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5332, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5341, Test Loss: 0.3138\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5341, Test Loss: 0.3135\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5228, Test Loss: 0.3132\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5253, Test Loss: 0.3134\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5270, Test Loss: 0.3133\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5331, Test Loss: 0.3133\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5410, Test Loss: 0.3151\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (25, 25, 2), 'lr': 0.001, 'weight_decay': 1e-05}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 4.4427, Test Loss: 0.3519\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.5590, Test Loss: 0.3208\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5319, Test Loss: 0.3137\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5272, Test Loss: 0.3135\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5350, Test Loss: 0.3135\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5268, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5159, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5311, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5249, Test Loss: 0.3137\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5436, Test Loss: 0.3145\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5327, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5358, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5250, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5302, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5301, Test Loss: 0.3138\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5395, Test Loss: 0.3135\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5295, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5271, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5199, Test Loss: 0.3132\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5417, Test Loss: 0.3141\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5199, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5394, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5251, Test Loss: 0.3133\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5369, Test Loss: 0.3136\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5379, Test Loss: 0.3135\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5363, Test Loss: 0.3139\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5099, Test Loss: 0.3145\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5336, Test Loss: 0.3134\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5333, Test Loss: 0.3133\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5312, Test Loss: 0.3132\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (25, 25, 2), 'lr': 0.0001, 'weight_decay': 0.01}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 96.1550, Test Loss: 0.4047\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.5928, Test Loss: 0.3236\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5357, Test Loss: 0.3138\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5320, Test Loss: 0.3133\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5217, Test Loss: 0.3133\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5295, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5388, Test Loss: 0.3136\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5294, Test Loss: 0.3136\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5242, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5374, Test Loss: 0.3135\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5308, Test Loss: 0.3136\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5206, Test Loss: 0.3134\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5408, Test Loss: 0.3135\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5206, Test Loss: 0.3135\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5255, Test Loss: 0.3134\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5270, Test Loss: 0.3134\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5286, Test Loss: 0.3134\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5381, Test Loss: 0.3135\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5381, Test Loss: 0.3137\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5247, Test Loss: 0.3136\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5213, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5385, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5229, Test Loss: 0.3134\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5356, Test Loss: 0.3134\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5256, Test Loss: 0.3135\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5296, Test Loss: 0.3135\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5375, Test Loss: 0.3135\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5393, Test Loss: 0.3136\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5315, Test Loss: 0.3136\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5176, Test Loss: 0.3134\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (25, 25, 2), 'lr': 0.0001, 'weight_decay': 0.0001}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 147.2229, Test Loss: 1.0849\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.8937, Test Loss: 0.3487\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.7771, Test Loss: 0.3291\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5557, Test Loss: 0.3235\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5485, Test Loss: 0.3165\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5346, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5280, Test Loss: 0.3184\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5395, Test Loss: 0.3246\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5310, Test Loss: 0.3282\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5405, Test Loss: 0.3247\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5606, Test Loss: 0.3167\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5324, Test Loss: 0.3186\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5367, Test Loss: 0.3145\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5406, Test Loss: 0.3199\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5885, Test Loss: 0.3156\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5410, Test Loss: 0.3190\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5288, Test Loss: 0.3276\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5309, Test Loss: 0.3254\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5364, Test Loss: 0.3202\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5503, Test Loss: 0.3262\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.6602, Test Loss: 0.3368\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5524, Test Loss: 0.3248\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5321, Test Loss: 0.3473\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.6105, Test Loss: 0.3380\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5932, Test Loss: 0.3322\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.6255, Test Loss: 0.3506\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.7131, Test Loss: 0.3123\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.6328, Test Loss: 0.5553\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.8067, Test Loss: 0.3182\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.6726, Test Loss: 0.4025\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (25, 25, 2), 'lr': 0.0001, 'weight_decay': 1e-05}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 15.6451, Test Loss: 0.5160\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.6931, Test Loss: 0.3710\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.5469, Test Loss: 0.3133\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.5308, Test Loss: 0.3133\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.5318, Test Loss: 0.3133\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.5276, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.5346, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5259, Test Loss: 0.3132\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.5313, Test Loss: 0.3133\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.5339, Test Loss: 0.3134\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5401, Test Loss: 0.3136\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5185, Test Loss: 0.3132\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5416, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5291, Test Loss: 0.3137\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5309, Test Loss: 0.3133\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5274, Test Loss: 0.3161\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5281, Test Loss: 0.3163\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5437, Test Loss: 0.3144\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5291, Test Loss: 0.3138\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5455, Test Loss: 0.3150\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5373, Test Loss: 0.3199\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5418, Test Loss: 0.3323\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5649, Test Loss: 0.4045\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.6005, Test Loss: 0.4290\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.6181, Test Loss: 0.3575\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.6188, Test Loss: 0.3198\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.7965, Test Loss: 0.6424\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.9901, Test Loss: 0.4763\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.6530, Test Loss: 0.3374\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.7221, Test Loss: 0.4327\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (25, 25, 2), 'lr': 1e-05, 'weight_decay': 0.01}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 160.9681, Test Loss: 1.2395\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 0.8968, Test Loss: 0.4332\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.7559, Test Loss: 0.4142\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.7343, Test Loss: 0.3973\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.6545, Test Loss: 0.3832\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.6314, Test Loss: 0.3714\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.6122, Test Loss: 0.3621\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.6832, Test Loss: 0.3543\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.6667, Test Loss: 0.3481\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.6179, Test Loss: 0.3429\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.5777, Test Loss: 0.3385\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.5687, Test Loss: 0.3347\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5769, Test Loss: 0.3315\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5565, Test Loss: 0.3289\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5491, Test Loss: 0.3267\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5570, Test Loss: 0.3247\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5478, Test Loss: 0.3231\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5532, Test Loss: 0.3220\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5411, Test Loss: 0.3209\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5394, Test Loss: 0.3199\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5371, Test Loss: 0.3190\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5368, Test Loss: 0.3182\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5477, Test Loss: 0.3177\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5424, Test Loss: 0.3172\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5335, Test Loss: 0.3168\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5465, Test Loss: 0.3163\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5327, Test Loss: 0.3161\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5313, Test Loss: 0.3158\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5359, Test Loss: 0.3154\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5328, Test Loss: 0.3152\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (25, 25, 2), 'lr': 1e-05, 'weight_decay': 0.0001}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 62.0797, Test Loss: 0.9324\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 1.2327, Test Loss: 0.7155\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 1.1243, Test Loss: 0.6511\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 1.0250, Test Loss: 0.5930\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.9475, Test Loss: 0.5432\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.8683, Test Loss: 0.5014\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.8056, Test Loss: 0.4671\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.7530, Test Loss: 0.4394\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.7124, Test Loss: 0.4171\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.6794, Test Loss: 0.3991\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.6538, Test Loss: 0.3845\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.6328, Test Loss: 0.3725\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.6138, Test Loss: 0.3627\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.5973, Test Loss: 0.3545\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.5890, Test Loss: 0.3477\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5789, Test Loss: 0.3423\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.5706, Test Loss: 0.3379\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5652, Test Loss: 0.3341\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5584, Test Loss: 0.3310\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5503, Test Loss: 0.3283\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.5570, Test Loss: 0.3263\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5444, Test Loss: 0.3245\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5453, Test Loss: 0.3229\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5411, Test Loss: 0.3215\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5372, Test Loss: 0.3204\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5483, Test Loss: 0.3195\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5370, Test Loss: 0.3187\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5284, Test Loss: 0.3179\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5353, Test Loss: 0.3173\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.5370, Test Loss: 0.3169\n",
      "Best parameter is updated to {'hidden_sizes': (25, 25, 2), 'lr': 1e-05, 'weight_decay': 0.0001}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing model for parameters {'hidden_sizes': (25, 25, 2), 'lr': 1e-05, 'weight_decay': 1e-05}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 3.7063, Test Loss: 2.4955\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 3.6696, Test Loss: 1.2766\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 2.1955, Test Loss: 1.0010\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 5.0713, Test Loss: 0.8697\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 1.1881, Test Loss: 0.7935\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 1.3156, Test Loss: 0.7270\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 1.4321, Test Loss: 0.6789\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.8975, Test Loss: 0.5927\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 1.0328, Test Loss: 0.5395\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 3.7496, Test Loss: 0.5478\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.7281, Test Loss: 0.5978\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.8711, Test Loss: 0.5201\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 3.2236, Test Loss: 0.4968\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.9565, Test Loss: 0.4411\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.6181, Test Loss: 0.4187\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.5990, Test Loss: 0.4175\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.8332, Test Loss: 0.3930\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.8350, Test Loss: 0.4069\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.6014, Test Loss: 0.4038\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.5613, Test Loss: 0.3832\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 2.7821, Test Loss: 0.3898\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.8364, Test Loss: 0.3837\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.5517, Test Loss: 0.3821\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 1.0180, Test Loss: 0.3860\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.7918, Test Loss: 0.3879\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5905, Test Loss: 0.3857\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 2.5145, Test Loss: 0.4132\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.7785, Test Loss: 0.5065\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 1.4519, Test Loss: 0.4837\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.7566, Test Loss: 0.4687\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# set initial best_accuracy to 0\n",
    "best_accuracy = 0\n",
    "# loop over all combinations of hyperparameters\n",
    "for params in param_list:\n",
    "    model, train_loader, test_loader, training_loss, testing_loss = train_model(params)\n",
    "    # evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    # Compute the training accuracy.\n",
    "    train_accuracy = accuracy(train_loader)\n",
    "    # update the best set of hyperparameters based on the validation accuracy\n",
    "    if train_accuracy > best_accuracy:\n",
    "        best_accuracy = train_accuracy\n",
    "        best_params = params\n",
    "        print(f'Best parameter is updated to {best_params}')\n",
    "\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08cfcd",
   "metadata": {},
   "source": [
    "## Train the model with best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6592700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model for parameters {'hidden_sizes': (25, 25, 2), 'lr': 1e-05, 'weight_decay': 0.0001}!\n",
      "Epoch [1/5], Step [200/1200], Train Loss: 11.1519, Test Loss: 1.9392\n",
      "Epoch [1/5], Step [400/1200], Train Loss: 1.1711, Test Loss: 0.5006\n",
      "Epoch [1/5], Step [600/1200], Train Loss: 0.7530, Test Loss: 0.4268\n",
      "Epoch [1/5], Step [800/1200], Train Loss: 0.6584, Test Loss: 0.4150\n",
      "Epoch [1/5], Step [1000/1200], Train Loss: 0.6751, Test Loss: 0.4136\n",
      "Epoch [1/5], Step [1200/1200], Train Loss: 0.8123, Test Loss: 0.4054\n",
      "Epoch [2/5], Step [200/1200], Train Loss: 0.6526, Test Loss: 0.4031\n",
      "Epoch [2/5], Step [400/1200], Train Loss: 0.5966, Test Loss: 0.4045\n",
      "Epoch [2/5], Step [600/1200], Train Loss: 0.7557, Test Loss: 0.4020\n",
      "Epoch [2/5], Step [800/1200], Train Loss: 0.6308, Test Loss: 0.4086\n",
      "Epoch [2/5], Step [1000/1200], Train Loss: 0.6027, Test Loss: 0.4125\n",
      "Epoch [2/5], Step [1200/1200], Train Loss: 0.7569, Test Loss: 0.4035\n",
      "Epoch [3/5], Step [200/1200], Train Loss: 0.5798, Test Loss: 0.4024\n",
      "Epoch [3/5], Step [400/1200], Train Loss: 0.7347, Test Loss: 0.4197\n",
      "Epoch [3/5], Step [600/1200], Train Loss: 0.6407, Test Loss: 0.4149\n",
      "Epoch [3/5], Step [800/1200], Train Loss: 0.6119, Test Loss: 0.4005\n",
      "Epoch [3/5], Step [1000/1200], Train Loss: 0.6578, Test Loss: 0.3865\n",
      "Epoch [3/5], Step [1200/1200], Train Loss: 0.5848, Test Loss: 0.3997\n",
      "Epoch [4/5], Step [200/1200], Train Loss: 0.5772, Test Loss: 0.4174\n",
      "Epoch [4/5], Step [400/1200], Train Loss: 0.6356, Test Loss: 0.4098\n",
      "Epoch [4/5], Step [600/1200], Train Loss: 0.6503, Test Loss: 0.4017\n",
      "Epoch [4/5], Step [800/1200], Train Loss: 0.5884, Test Loss: 0.4012\n",
      "Epoch [4/5], Step [1000/1200], Train Loss: 0.7252, Test Loss: 0.3862\n",
      "Epoch [4/5], Step [1200/1200], Train Loss: 0.5748, Test Loss: 0.3903\n",
      "Epoch [5/5], Step [200/1200], Train Loss: 0.5563, Test Loss: 0.3875\n",
      "Epoch [5/5], Step [400/1200], Train Loss: 0.5768, Test Loss: 0.3939\n",
      "Epoch [5/5], Step [600/1200], Train Loss: 0.5929, Test Loss: 0.3896\n",
      "Epoch [5/5], Step [800/1200], Train Loss: 0.5930, Test Loss: 0.3889\n",
      "Epoch [5/5], Step [1000/1200], Train Loss: 0.5909, Test Loss: 0.3872\n",
      "Epoch [5/5], Step [1200/1200], Train Loss: 0.6934, Test Loss: 0.3977\n"
     ]
    }
   ],
   "source": [
    "# train the model with best hyperparameters\n",
    "params = best_params\n",
    "model, train_loader, test_loader, training_loss, testing_loss = train_model(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b2dcc4",
   "metadata": {},
   "source": [
    "## Plot training loss and testing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a6e0e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Remove the last training loss value (it might be incomplete).\n",
    "training_loss.pop()\n",
    "# Insert a NaN value at the beginning of the testing loss list (to shift the plot).\n",
    "testing_loss.insert(0, float('NaN'))\n",
    "\n",
    "# Plot the training and testing losses.\n",
    "plt.plot(training_loss, label=\"training loss\")\n",
    "plt.plot(testing_loss, label=\"testing loss\")\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Add vertical lines to show the end of each epoch.\n",
    "epochs = 5\n",
    "batch_size = 20\n",
    "for i in range(epochs):\n",
    "    plt.axvline(x=(len(xs_train) // batch_size) * (i + 1) / 200, color='gray')\n",
    "plt.title('Training and testing loss by batch during model training')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51602e6c",
   "metadata": {},
   "source": [
    "As shown in the figure above, the training loss and the testing loss quickly converges,then remains almost consistent. Thus, the training is successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6d225",
   "metadata": {},
   "source": [
    "## Compute the train accuracy and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a25c68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy of the MLP: 0.778\n",
      "Test accuracy of the MLP: 0.784\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode.\n",
    "model.eval()\n",
    "# Compute the training accuracy.\n",
    "train_accuracy = accuracy(train_loader)\n",
    "print('Train accuracy of the MLP: {:.3f}'.format(train_accuracy))\n",
    "# Compute the testing accuracy.\n",
    "test_accuracy = accuracy(test_loader)\n",
    "print('Test accuracy of the MLP: {:.3f}'.format(test_accuracy))\n",
    "# Name the accuracy for plotting\n",
    "accuracy2 = test_accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c8447",
   "metadata": {},
   "source": [
    "The accuracy increase here is not considerable, which is possiblly resulted by the properties of dataset itself. Besides, activation function, optimizer and criterium may not be the most suitable one in that model. In the following part, the model in scikit-learn package is adopted, which provides more flexibity of hyper parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd15ccf",
   "metadata": {},
   "source": [
    "## Use scikit-learn package to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee87fc",
   "metadata": {},
   "source": [
    "There is a mature MLP classifier model in scikit-learn library, which allow us to further tune the hyper parameter (even activation function and optimizer). (Reference from https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "236e7f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found by GridSearchCV:\n",
      "{'activation': 'tanh', 'hidden_layer_sizes': (20, 20), 'learning_rate_init': 0.0001, 'solver': 'adam'}\n",
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "xs_train = scaler.fit_transform(xs_train)\n",
    "xs_test = scaler.transform(xs_test)\n",
    "\n",
    "# Define the MLPClassifier\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "\n",
    "# Set up hyperparameters for GridSearchCV\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(15,15), (20, 20), (25, 25)], # Different hidden layer configurations\n",
    "    'activation': ['relu', 'tanh'], # Activation functions\n",
    "    'solver': ['adam', 'sgd'], # Optimizers\n",
    "    'learning_rate_init': [0.00001, 0.0001, 0.001], # Initial learning rates\n",
    "}\n",
    "\n",
    "# Define the GridSearchCV object\n",
    "grid_search = GridSearchCV(mlp_classifier, param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "\n",
    "# Train the MLPClassifier using GridSearchCV on the training data\n",
    "grid_search.fit(xs_train, ys_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters found by GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Make predictions on the testing data using the best estimator\n",
    "ys_pred = grid_search.best_estimator_.predict(xs_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(ys_test, ys_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "# Name the accuracy for plotting\n",
    "accuracy3 = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00bff1",
   "metadata": {},
   "source": [
    "Fortunately, there is no best hyperparameters on the boundary. And the accuracy rate has improved significantly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56cfe9",
   "metadata": {},
   "source": [
    "## Plot the accuracy throughout the model improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40768525",
   "metadata": {},
   "source": [
    "This plot can intuitively show the accuracy improvement throughout trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "234d7257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define dataset to plot\n",
    "x_axis = ['Model 1', 'Model 2', 'Model 3']\n",
    "y_axis = [accuracy1, accuracy2, accuracy3]\n",
    "\n",
    "bar_width = 0.2\n",
    "\n",
    "# Find the index of the highest value\n",
    "highest_value_index = y_axis.index(max(y_axis))\n",
    "\n",
    "# Create a color list and set a different color for the highest value\n",
    "colors = ['blue'] * len(x_axis)\n",
    "colors[highest_value_index] = 'orange'\n",
    "plt.ylim(0.6,0.9)\n",
    "# Create a bar chart\n",
    "plt.bar(x_axis, y_axis, width=bar_width, color=colors)\n",
    "\n",
    "# Display the value above each bar\n",
    "for i, value in enumerate(y_axis):\n",
    "    plt.text(i, value+0.005, str(value), ha='center', va='bottom')\n",
    "             \n",
    "# Add title and labels for the axes\n",
    "plt.title('Accuracy for each model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ccfabe",
   "metadata": {},
   "source": [
    "## Save the trained model to pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643e023",
   "metadata": {},
   "source": [
    "Save the best MLP model for future use (Referenced from: https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0bc11d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the best model from the file\\nwith open(\"MLP_sklearn.sav\", \"rb\") as f:\\n    loaded_model = pickle.load(f)\\n\\n# Use the loaded model for prediction\\nys_pred = loaded_model.predict(xs_test)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the best model to a file\n",
    "with open(\"MLP_sklearn.sav\", \"wb\") as f:\n",
    "    pickle.dump(grid_search.best_estimator_, f)\n",
    "    \n",
    "'''\n",
    "# Load the best model from the file\n",
    "with open(\"MLP_sklearn.sav\", \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Use the loaded model for prediction\n",
    "ys_pred = loaded_model.predict(xs_test)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
